{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.read.format(\"com.databricks.spark.avro\").option(\"header\",\"true\").load(\"/../part-m-00001.avro\").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.arange(1,5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# посмотрим на все таблицы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=spark.read.format(\"com.databricks.spark.avro\").option(\"header\", \"true\").load(\"/\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.registerTempTable(\"table\")\n",
    "spark.sql(\"select operatorid, created, sendername, data from table\").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+----------+----+\n",
      "|operatorid|      created|sendername|data|\n",
      "+----------+-------------+----------+----+\n",
      "|      null|1566774017000|   Евгений|null|\n",
      "+----------+-------------+----------+----+\n",
      "only showing top 1 row"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select operatorid, count(*) as cnt from table group by operatorid order by cnt desc limit 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.read.format(\"com.databricks.spark.avro\").option(\"header\", \"true\").load(\"/.../part-m-00000.avro\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp_df=spark.read.format(\"com.databricks.spark.avro\").option(\"header\", \"true\").load(\"/.../part-m-00001.avro\")\n",
    "temp_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp_df.registerTempTable(\"table2\")\n",
    "spark.sql(\"select visitsessionid from table2 limit 3\").show(3, False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select json, count(json) as cnt from table2 group by json order by cnt desc\").show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|token|\n",
      "+-----+\n",
      "|    1|\n",
      "+-----+\n",
      "only showing top 1 row"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select token from table2\").show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=spark.read.format(\"com.databricks.spark.avro\").option(\"header\", \"true\").load(\"/.../\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+-------+---------------+----+\n",
      "|invitationstate|invitationid|country|region         |city|\n",
      "+---------------+------------+-------+---------------+----+\n",
      "|null           |null        |Россия |Санкт-Петербург|null|\n",
      "|null           |null        |Россия |Одинцово       |null|\n",
      "|null           |null        |Россия |Чита           |null|\n",
      "|null           |null        |Россия |Москва         |null|\n",
      "|null           |null        |null   |null           |null|\n",
      "|null           |null        |Россия |Москва         |null|\n",
      "|null           |null        |Россия |Москва         |null|\n",
      "|null           |null        |null   |null           |null|\n",
      "|null           |null        |Россия |Москва         |null|\n",
      "|null           |null        |Россия |Челябинск      |null|\n",
      "+---------------+------------+-------+---------------+----+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "df.registerTempTable('table3')\n",
    "spark.sql('select invitationstate, invitationid, country, region, city  from table3').show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### выгрузим данные по именам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parquetFile = spark.read.parquet(\"/.../part-00000-..c000.snappy.parquet\")\n",
    "parquetFile.createOrReplaceTempView(\"parquetFile\")\n",
    "temp = spark.sql(\"SELECT * from parquetFile limit 10\")\n",
    "temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|     name|   cnt|\n",
      "+---------+------+\n",
      "|     null|495699|\n",
      "|Александр| 53194|\n",
      "|   Сергей| 47877|\n",
      "|    Елена| 47540|\n",
      "|  Татьяна| 45232|\n",
      "+---------+------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "names_df=spark.sql(\"SELECT firstname as name, count(*) as cnt from parquetFile group by name order by cnt desc\")\n",
    "names_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names_df.repartition(1).write.format('com.databricks.spark.csv').save(\"/user/temp.csv\",header = 'true')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
