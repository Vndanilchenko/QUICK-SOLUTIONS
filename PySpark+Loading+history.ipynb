{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\"conf\": {\"spark.driver.extraClassPath\": \"/hadoop/smartdata/lib/spark-avro_2.11-2.4.0.cloudera2.jar\",\n",
    "          \"spark.executor.extraClassPath\": \"/hadoop/smartdata/lib/spark-avro_2.11-2.4.0.cloudera2.jar\"\n",
    "                   },\n",
    " \"proxyUser\": \"username\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(1, 5):\n",
    "    print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"com.databricks.spark.avro\").option(\"header\",\"true\").load(\"/*\")\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37817"
     ]
    }
   ],
   "source": [
    "# –ø–æ—Å—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# –ø–æ—Å–º–æ—Ç—Ä–∏–º —Ç–∏–ø—ã –ø–æ–ª—è\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|summary|             message|\n",
      "+-------+--------------------+\n",
      "|  count|               37817|\n",
      "|   mean|1.586374255162066...|\n",
      "| stddev|8.087341974639279...|\n",
      "|    min|                    |\n",
      "|    max|ü§¶üèΩ‚Äç‚ôÄÔ∏è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç...|\n",
      "+-------+--------------------+"
     ]
    }
   ],
   "source": [
    "df.describe(\"message\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# –≤—ã–±–µ—Ä–µ–º —Ç–æ–ø 20 —Å–æ–æ–±—â–µ–Ω–∏–π\n",
    "df.select(\"messageid\", \"message\", \"operatorid\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# –æ—Ç—Ñ–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –æ–ø–µ—Ä–∞—Ç–æ—Ä—É —Å –Ω–æ–º–µ—Ä–æ–º 176283 (—á–∞—Ç-–±–æ—Ç)\n",
    "df.select(\"messageid\", \"message\").filter(\"operatorid==176283\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"table1\")\n",
    "df2 = spark.sql(\"SELECT messageid, message from table1 limit 20\")\n",
    "df2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# –æ—Ç–æ–±—Ä–∞–∑–∏–º –≤ —Ç–∞–±–ª–∏—á–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ\n",
    "df2=spark.sql(\"SELECT messageid, message from table1 limit 20\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# –≤—ã–≤–µ–¥–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ—Ç–∏–ø–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –±–æ—Ç–∞\n",
    "spark.sql(\"SELECT message as col1, count('') as cnt \\\n",
    "          from table1 \\\n",
    "          where operatorid=176283 \\\n",
    "          group by message \\\n",
    "          order by cnt desc \\\n",
    "          limit 20\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "—Å–æ—Ö—Ä–∞–Ω–∏–º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%spark -o df\n",
    "df = spark.read.format(\"com.databricks.spark.avro\").option(\"header\",\"true\").load(\"/../part-m-00001.avro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "df.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df.repartition(1).write.format(\"com.databricks.spark.csv\").save(\"fileOutput_names.csv\",header = 'true')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
